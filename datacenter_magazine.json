{"avis":["Pendant six heures, Facebook, Instagram et WhatsApp ont été non pas en panne, ni hors ligne, mais simplement introuvables. Que s’est-il passé ? Facebook confirme qu’il s’agit d’erreurs en cascade qui ont provoqué la panne mondiale des services… et lui ont fait perdre 6 milliards de dollars en bourse.","Ironiquement, ce sont les règles de sécurité qui ralentissent les équipes pour accéder aux bâtiments et aux serveurs.","Autre enseignement de cette panne massive, les outils utilisés pour superviser et résoudre les problèmes étaient devenus inaccessible suite à la perte du DNS.","Pour Facebook, il faudra rapidement mettre en place de nouvelles procédures pour réactiver plus rapidement les services et surtout éviter la perte totale du DNS qui lance le domino. Comme quoi, quand nous parlons de l’importance d’un PRA et des plans de secours, ce n’est pas un hasard.","Blog de Facebook sur la panne : https://engineering.fb.com/2021/10/05/networking-traffic/outage-details/","Mercredi 13 octobre, OVH a subi une panne sur l'ensemble du backbone suite à une erreur humaine durant la configuration du réseau sur un datacenter situé sur la côte Est américaine. Octave Klaba a rapidement donné des nouvelles, fidèle à son habitude. Il a précisé que le datacenter avait été…","La pépite française EfficientIP annonce une solution de routage du trafic qui optimise les temps de réponse des applications, et maximise l’expérience utilisateur ainsi que la résilience. Elle est présentée comme une avancée majeure pour les organisations multi-sites. EfficientIP, fournisseur de solutions de sécurité et d’automatisation réseau, spécialisé en gestion DDI…","Nous faisons le point sur l'avancée du plan de reprise d'OVHcloud sur le site incendié de Strasbourg. Rappel de nos précédents articles : 18/03 - Incendie – 6 erreurs d’OVHcloud18/03 - Le Gouvernement soutiendra OVHcloud17/03 - Après l’incendie d’OVHcloud, les clients font face à la réalité du cloud16/03 - Incendie…","Les podcast Radio DSI, Radio DCmag et les micro-infographies Datacenter Magazine sont de nouveaux modes de contenu proposés sous licence Creative Commons CC BY-ND (Creative Commons Attribution-NoDerivatives 4.0 International Public License), et destinées à alimenter les sites, blogs et documents produits par et pour l’écosystème des datacenters.\n","ACTUALITÉS\nAfrique\nChroniques – Tribunes\nDatacenter Actu\nDatacenter Colocation, Opérateurs & Hébergement\nDatacenter Produits & Services\nAdmin & Management\nEdge\nEnergie\nImmobilier\nInfrastructure\nInterconnexions\nRefroidissement / Cooling\nInnovation\nIT Gouvernance & Management\nIT Infrastructure et Cloud\nCloud computing\nHPC & Calcul\nMainframe\nSécurité\nTendances & Etudes","Datacenter Colocation, Opérateurs & Hébergement","Datacenter Produits & Services\nAdmin & Management\nEdge\nEnergie\nImmobilier\nInfrastructure\nInterconnexions\nRefroidissement / Cooling","IT Gouvernance & Management","IT Infrastructure et Cloud\nCloud computing\nHPC & Calcul","LIVRES BLANCS & PUBLICATIONS","Etape 1 : une opération de maintenance de routine, avec des commandes serveur (rien que de plus normal pour ce genre de maintenance), fait tomber les serveurs DNS qui deviennent inaccessible à l’infrastructure. L’ensemble du backbone tombe par effet domino.","Etape 2 : la perte du DNS entraîne la perte du reste des serveurs et les équipes ne peuvent pas accéder à distance aux serveurs et aux éléments de l’infrastructure pour remettre en état le réseau","Etape 3 : les équipes doivent se déplacer physiquement dans les datacenters pour remettre en place le réseau, le DNS et reconnecter les serveurs","Etape 4 : la restauration des serveurs s’est faite par étape pour éviter un problème d’alimentation électrique avec un bond de la consommation alors que la panne avait provoqué une baisse de la consommation."]}